{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mXUeIpkZNOVa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "gnQHM7mmOXbt",
    "outputId": "cdad9180-b42e-4de1-c31e-00d540113739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently selected TF version: 2.x\n",
      "Available versions:\n",
      "* 1.x\n",
      "* 2.x\n"
     ]
    }
   ],
   "source": [
    "tensorflow_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naUxyxloNhe_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    \n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "GrO-UEfNN42f",
    "outputId": "ce7bf3c4-e845-492b-f5ab-b655eb0877db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3RjtTWnOD6J"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/drive/My Drive/Transformer/english.en\",mode = 'r',encoding='utf-8')as f:\n",
    "    english = f.read()\n",
    "\n",
    "with open(\"/content/drive/My Drive/Transformer/spanish.es\",mode='r',encoding='utf-8')as f:\n",
    "    spanish = f.read() \n",
    "\n",
    "with open(\"/content/drive/My Drive/Transformer/nonbreaking_prefix.en\",mode='r',encoding='utf-8')as f:\n",
    "    nonbreaking_english = f.read() \n",
    "\n",
    "with open(\"/content/drive/My Drive/Transformer/nonbreaking_prefix.es\",mode='r',encoding='utf-8')as f:\n",
    "    nonbreaking_spanish = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eXmQkbYDP1EI",
    "outputId": "e5d3939a-ccf5-412f-afee-98a3146a5333"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session\\nI declare resumed the se'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "aIIXm48OPPn0",
    "outputId": "df85509b-1ac7-4867-dec8-fed445f06af2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reanudación del período de sesiones\\nDeclaro reanud'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spanish[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "b0ajoY-4P6D9",
    "outputId": "335dab17-3b23-4b03-af67-ac4e386c1bd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nA\\nB\\nC\\nD\\nE\\nF\\nG\\nH\\nI\\nJ\\nK\\nL\\nM\\nN\\nO\\nP\\nQ\\nR\\nS\\nT\\nU\\nV\\nW\\nX\\nY\\nZ\\nAdj\\nAdm\\nAdv\\nAsst\\nBart\\nBldg\\nBrig\\nBros\\nCapt\\nCmdr\\nCol\\nComdr\\nCon\\nCorp\\nCpl\\nDR\\nDr\\nDrs\\nEns\\nGen\\nGov\\nHon\\nHr\\nHosp\\nInsp\\nLt\\nMM\\nMR\\nMRS\\nMS\\nMaj\\nMessrs\\nMlle\\nMme\\nMr\\nMrs\\nMs\\nMsgr\\nOp\\nOrd\\nPfc\\nPh\\nProf\\nPvt\\nRep\\nReps\\nRes\\nRev\\nRt\\nSen\\nSens\\nSfc\\nSgt\\nSr\\nSt\\nSupt\\nSurg\\nv\\nvs\\ni.e\\nrev\\ne.g\\nRs\\nNo \\nNos\\nArt\\nNr\\npp \\nJan\\nFeb\\nMar\\nApr\\nJun\\nJul\\nAug\\nSep\\nOct\\nNov\\nDec\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonbreaking_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0oglh5uatu6"
   },
   "outputs": [],
   "source": [
    "nonbreaking_english = nonbreaking_english.split(\"\\n\")\n",
    "nonbreaking_english = [\" \" + prefix + \".\" for prefix in nonbreaking_english]\n",
    "\n",
    "nonbreaking_spanish = nonbreaking_spanish.split(\"\\n\")\n",
    "nonbreaking_spanish = [\" \" + prefix + \".\" for prefix in nonbreaking_spanish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "A67SXM1BcKQy",
    "outputId": "55dcce79-bda1-4bcd-c60c-75c8238d6ace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' .',\n",
       " ' A.',\n",
       " ' B.',\n",
       " ' C.',\n",
       " ' D.',\n",
       " ' E.',\n",
       " ' F.',\n",
       " ' G.',\n",
       " ' H.',\n",
       " ' I.',\n",
       " ' J.',\n",
       " ' K.',\n",
       " ' L.',\n",
       " ' M.',\n",
       " ' N.',\n",
       " ' O.',\n",
       " ' P.',\n",
       " ' Q.',\n",
       " ' R.',\n",
       " ' S.',\n",
       " ' T.',\n",
       " ' U.',\n",
       " ' V.',\n",
       " ' W.',\n",
       " ' X.',\n",
       " ' Y.',\n",
       " ' Z.',\n",
       " ' Adj.',\n",
       " ' Adm.',\n",
       " ' Adv.',\n",
       " ' Asst.',\n",
       " ' Bart.',\n",
       " ' Bldg.',\n",
       " ' Brig.',\n",
       " ' Bros.',\n",
       " ' Capt.',\n",
       " ' Cmdr.',\n",
       " ' Col.',\n",
       " ' Comdr.',\n",
       " ' Con.',\n",
       " ' Corp.',\n",
       " ' Cpl.',\n",
       " ' DR.',\n",
       " ' Dr.',\n",
       " ' Drs.',\n",
       " ' Ens.',\n",
       " ' Gen.',\n",
       " ' Gov.',\n",
       " ' Hon.',\n",
       " ' Hr.',\n",
       " ' Hosp.',\n",
       " ' Insp.',\n",
       " ' Lt.',\n",
       " ' MM.',\n",
       " ' MR.',\n",
       " ' MRS.',\n",
       " ' MS.',\n",
       " ' Maj.',\n",
       " ' Messrs.',\n",
       " ' Mlle.',\n",
       " ' Mme.',\n",
       " ' Mr.',\n",
       " ' Mrs.',\n",
       " ' Ms.',\n",
       " ' Msgr.',\n",
       " ' Op.',\n",
       " ' Ord.',\n",
       " ' Pfc.',\n",
       " ' Ph.',\n",
       " ' Prof.',\n",
       " ' Pvt.',\n",
       " ' Rep.',\n",
       " ' Reps.',\n",
       " ' Res.',\n",
       " ' Rev.',\n",
       " ' Rt.',\n",
       " ' Sen.',\n",
       " ' Sens.',\n",
       " ' Sfc.',\n",
       " ' Sgt.',\n",
       " ' Sr.',\n",
       " ' St.',\n",
       " ' Supt.',\n",
       " ' Surg.',\n",
       " ' v.',\n",
       " ' vs.',\n",
       " ' i.e.',\n",
       " ' rev.',\n",
       " ' e.g.',\n",
       " ' Rs.',\n",
       " ' No .',\n",
       " ' Nos.',\n",
       " ' Art.',\n",
       " ' Nr.',\n",
       " ' pp .',\n",
       " ' Jan.',\n",
       " ' Feb.',\n",
       " ' Mar.',\n",
       " ' Apr.',\n",
       " ' Jun.',\n",
       " ' Jul.',\n",
       " ' Aug.',\n",
       " ' Sep.',\n",
       " ' Oct.',\n",
       " ' Nov.',\n",
       " ' Dec.',\n",
       " ' .']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonbreaking_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJLNkCq7cMSK"
   },
   "outputs": [],
   "source": [
    "corpus_en = english\n",
    "## most of cleaning part is done by Tokenizer we will be using\n",
    "# What we must do is remove '.' from non ending words such as i.e. or a.m.\n",
    "for prefix in nonbreaking_english:\n",
    "    \n",
    "    corpus_en = corpus_en.replace(prefix, prefix+\"$$$\")\n",
    "  # here now nonending \".\" will be denoted by $$$ at last\n",
    "\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[A-Z]|[a-z])\",\"$$$\",corpus_en)\n",
    "#\".\" have separate meaning in regex so we have to use \"\\\" before it\n",
    "#\"?=\" means check for the regular expression but dont replace \"|\"== or\n",
    "# now we also want to add $$$ if \".\" is not followed by space as it doesnt indicate end\n",
    "\n",
    "corpus_en = re.sub(r\"\\.$$$\",\" \",corpus_en)\n",
    "corpus_en = re.sub(r\"  +\",\" \",corpus_en)\n",
    "corpus_en = corpus_en.split(\"\\n\")\n",
    "\n",
    "corpus_es = spanish\n",
    "for prefix in nonbreaking_spanish:\n",
    "    corpus_es = corpus_es.replace(prefix, prefix+\"$$$\")\n",
    "corpus_es = re.sub(r\"\\.(?=[0-9]|[A-Z]|[a-z])\",\"$$$\",corpus_es)\n",
    "corpus_es = re.sub(r\"\\.$$$\",\" \",corpus_es)\n",
    "corpus_es = re.sub(r\"  +\",\" \",corpus_es)\n",
    "corpus_es = corpus_es.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "q1vydaWjg9-M",
    "outputId": "d38c9748-f396-4fa3-ce89-c88dd8767b60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Resumption of the session',\n",
       " 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       " \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
       " 'You have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
       " \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\",\n",
       " \"Please rise, then, for this minute' s silence.\",\n",
       " \"(The House rose and observed a minute' s silence)\",\n",
       " 'Madam President, on a point of order.',\n",
       " 'You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.',\n",
       " 'One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.',\n",
       " \"Would it be appropriate for you, Madam President, to write a letter to the Sri Lankan President expressing Parliament's regret at his and the other violent deaths in Sri Lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation?\",\n",
       " 'Yes, Mr Evans, I feel an initiative of the type you have just suggested would be entirely appropriate.',\n",
       " 'If the House agrees, I shall do as Mr Evans has suggested.',\n",
       " 'Madam President, on a point of order.',\n",
       " 'I would like your advice about Rule 143 concerning inadmissibility.',\n",
       " 'My question relates to something that will come up on Thursday and which I will then raise again.',\n",
       " 'The Cunha report on multiannual guidance programmes comes before Parliament on Thursday and contains a proposal in paragraph 6 that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually.',\n",
       " 'It says that this should be done despite the principle of relative stability.',\n",
       " 'I believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible.',\n",
       " 'I want to know whether one can raise an objection of that kind to what is merely a report, not a legislative proposal, and whether that is something I can competently do on Thursday.',\n",
       " 'That is precisely the time when you may, if you wish, raise this question, i$$$e.$$$ on Thursday prior to the start of the presentation of the report.',\n",
       " \"Madam President, coinciding with this year' s first part-session of the European Parliament, a date has been set, unfortunately for next Thursday, in Texas in America, for the execution of a young 34 year-old man who has been sentenced to death. We shall call him Mr Hicks.\",\n",
       " 'At the request of a French Member, Mr Zimeray, a petition has already been presented, which many people signed, including myself. However, I would ask you, in accordance with the line which is now constantly followed by the European Parliament and by the whole of the European Community, to make representations, using the weight of your prestigious office and the institution you represent, to the President and to the Governor of Texas, Mr Bush, who has the power to order a stay of execution and to reprieve the condemned person.',\n",
       " 'This is all in accordance with the principles that we have always upheld.',\n",
       " 'Thank you, Mr Segni, I shall do so gladly.',\n",
       " 'Indeed, it is quite in keeping with the positions this House has always adopted.',\n",
       " 'Madam President, I should like to draw your attention to a case in which this Parliament has consistently shown an interest.',\n",
       " 'It is the case of Alexander Nikitin.',\n",
       " 'All of us here are pleased that the courts have acquitted him and made it clear that in Russia, too, access to environmental information is a constitutional right.',\n",
       " 'Now, however, he is to go before the courts once more because the public prosecutor is appealing.',\n",
       " 'We know, and we have stated as much in very many resolutions indeed, including specifically during the last plenary part-session of last year, that this is not solely a legal case and that it is wrong for Alexander Nikitin to be accused of criminal activity and treason because of our involvement as the beneficiaries of his findings.',\n",
       " \"These findings form the basis of the European programmes to protect the Barents Sea, and that is why I would ask you to examine a draft letter setting out the most important facts and to make Parliament's position, as expressed in the resolutions which it has adopted, clear as far as Russia is concerned.\",\n",
       " 'Yes, Mrs Schroedter, I shall be pleased to look into the facts of this case when I have received your letter.',\n",
       " 'Madam President, I would firstly like to compliment you on the fact that you have kept your word and that, during this first part-session of the new year, the number of television channels in our offices has indeed increased considerably.',\n",
       " 'But, Madam President, my personal request has not been met.',\n",
       " 'Although there are now two Finnish channels and one Portuguese one, there is still no Dutch channel, which is what I had requested because Dutch people here like to be able to follow the news too when we are sent to this place of exile every month.',\n",
       " 'I would therefore once more ask you to ensure that we get a Dutch channel as well.',\n",
       " \"Mrs Plooij-van Gorsel, I can tell you that this matter is on the agenda for the Quaestors' meeting on Wednesday.\",\n",
       " 'It will, I hope, be examined in a positive light.',\n",
       " 'Madam President, can you tell me why this Parliament does not adhere to the health and safety legislation that it actually passes?',\n",
       " 'Why has no air quality test been done on this particular building since we were elected?',\n",
       " 'Why has there been no Health and Safety Committee meeting since 1998?',\n",
       " 'Why has there been no fire drill, either in the Brussels Parliament buildings or the Strasbourg Parliament buildings?',\n",
       " 'Why are there no fire instructions?',\n",
       " 'Why have the staircases not been improved since my accident?',\n",
       " 'Why are no-smoking areas not enforced?',\n",
       " 'It seems absolutely disgraceful that we pass legislation and do not adhere to it ourselves.',\n",
       " 'Mrs Lynne, you are quite right and I shall check whether this has actually not been done.',\n",
       " 'I shall also refer the matter to the College of Quaestors, and I am certain that they will be keen to ensure that we comply with the regulations we ourselves vote on.',\n",
       " 'Madam President, Mrs Díez González and I had tabled questions on certain opinions of the Vice-President, Mrs de Palacio, which appeared in a Spanish newspaper.',\n",
       " 'The competent services have not included them in the agenda on the grounds that they had been answered in a previous part-session.',\n",
       " 'I would ask that they reconsider, since this is not the case.',\n",
       " \"The questions answered previously referred to Mrs de Palacio' s intervention, on another occasion, and not to these comments which appeared in the ABC newspaper on 18 November.\",\n",
       " 'Mr Berenguer Fuster, we shall check all this.',\n",
       " 'I admit that, at present, the matter seems to be somewhat confused.',\n",
       " 'We shall therefore look into it properly to ensure that everything is as it should be.',\n",
       " \"Madam President, I should like to know if there will be a clear message going out from Parliament this week about our discontent over today's decision refusing to renew the arms embargo on Indonesia, considering that the vast majority in this Parliament have endorsed the arms embargo in Indonesia in the past?\",\n",
       " \"Today's decision not to renew the embargo is extremely dangerous considering the situation there.\",\n",
       " 'So Parliament should send a message, since that is the wish of the vast majority.',\n",
       " 'It is irresponsible of EU Member States to refuse to renew the embargo.',\n",
       " 'As people have said, the situation there is extremely volatile.',\n",
       " 'There is, in fact, a risk of a military coup in the future.',\n",
       " 'We do not know what is happening.',\n",
       " 'So why should EU arms producers profit at the expense of innocent people?',\n",
       " 'In any event, this question is not presently included among the requests for topical and urgent debate on Thursday.',\n",
       " 'Agenda',\n",
       " 'The next item is the verification of the final version of the draft agenda as drawn up by the Conference of Presidents at its meeting of 13 January pursuant to Rule 110 of the Rules of Procedure.',\n",
       " 'No amendments have been proposed relating to Monday and Tuesday.',\n",
       " 'Relating to Wednesday:',\n",
       " 'The Group of the Party of European Socialists requests that a Commission statement be included on its strategic objectives for the next five years and on the administrative reform of the Commission.',\n",
       " 'I would like Mr Barón Crespo, who made the request, to speak to propose it. That is, if he so wishes, of course.',\n",
       " 'Then we shall follow the usual procedure, hearing one speaker in favour and one against.',\n",
       " \"Madam President, the presentation of the Prodi Commission' s political programme for the whole legislature was initially a proposal by the Group of the Party of European Socialists which was unanimously approved by the Conference of Presidents in September and which was also explicitly accepted by President Prodi, who reiterated his commitment in his inaugural speech.\",\n",
       " \"This commitment is important because the Commission is a body with a monopoly of initiative in accordance with the Treaties and, therefore, basically dictates this Parliament' s political and legislative activity for the next five years.\",\n",
       " 'I would also like to point out, Madam President, that this Parliament voted to express its confidence in President Prodi during the previous legislature. It did so again during this legislature, in July, and then, in September, it voted once more to approve the whole Commission.',\n",
       " 'There has therefore been enough time for the Commission to prepare its programme and for us to become familiar with it and explain it to our citizens.',\n",
       " 'To this end, I would like to remind you of the resolution of 15 September, which recommended that the proposal be presented as soon as possible.',\n",
       " 'The events of last week - which originated outside the Conference of Presidents, that Conference being used simply to corroborate and ratify decisions taken elsewhere - present us with a dilemma. Either the Commission is not ready to present this programme, in which case it should clarify it.',\n",
       " 'According to its President, it is in a position to do so.',\n",
       " 'Given that the Commission is represented by Vice-President de Palacio, I believe that, before voting, it would help if the Commission could let us know how ready it is to present this programme, as agreed. Alternatively, Parliament is not ready to examine this programme, as some appear to be suggesting.',\n",
       " \"In my opinion, this second hypothesis would imply the failure of Parliament in its duty as a Parliament, as well as introducing an original thesis, an unknown method which consists of making political groups aware, in writing, of a speech concerning the Commission' s programme a week earlier - and not a day earlier, as had been agreed - bearing in mind that the legislative programme will be discussed in February, so we could forego the debate, since on the next day our citizens will hear about it in the press and on the Internet and Parliament will no longer have to worry about it.\",\n",
       " 'My Group believes that since a parliament is meant to listen, debate and reflect, there can be no justification whatsoever for this delay and we believe that, if the Commission is ready to do so, we still have time to re-establish the original agreement between Parliament and the Commission and proceed in a manner which fulfils our duty to our fellow citizens.',\n",
       " 'Therefore, the proposal of the Group of the Party of European Socialists, and which you have mentioned, is that the Prodi Commission present its legislative programme on Wednesday, including its proposed administrative reform, because, otherwise, we could find ourselves in a paradoxical situation: on the pretext that there is no text, on the one hand, the President of the Commission would be denied his right to speak in this Parliament and, on the other hand, there would be a debate on a reform when Parliament had no prior knowledge of the texts on which it is based.',\n",
       " 'Therefore, Madam President, I would ask you to request that the Commission express its opinion on this issue and that we then proceed to the vote.',\n",
       " '(Applause from the PSE Group)',\n",
       " 'Madam President, I would like to make it very clear that, above all, the Commission has absolute respect for the decisions of this Parliament and, amongst those, the decision establishing its agenda.',\n",
       " 'We therefore respect whatever Parliament may decide.',\n",
       " \"But I would also like to make it very clear that President Prodi made a commitment to this Parliament to introduce a new debate, as Mr Barón Crespo has reminded us, which would be in addition to the annual debate on the Commission' s legislative programme, on the broad areas of action for the next five years, that is to say, for this legislature.\",\n",
       " \"Madam President, I would like to say that the agreement reached in September distinguished this debate from the annual presentation of the Commission' s legislative programme.\",\n",
       " 'I would also like to say that the Commission is prepared and ready to hold this debate whenever it is convenient and that we were ready to do so this week as we had agreed originally, on the basis that it would be presented the day before in a speech to parliamentary groups.',\n",
       " 'Therefore, Madam President, I would like to repeat that the Commission has debated the action plan for the next five years and, when Parliament decides, - this week if that is the decision - we are prepared to come and explain the programme for the next five years and, next month, the programme for 2000, which is what we fully agreed upon.',\n",
       " 'I propose that we vote on the request of the Group of the Party of European Socialists that the Commission statement on its strategic objectives should be reinstated.',\n",
       " '(Parliament rejected the request) President.',\n",
       " \"Still on the subject of Wednesday' s sitting, I have another proposal regarding the oral question on capital tax.\",\n",
       " 'The PPE-DE Group is requesting that this item be taken off the agenda.',\n",
       " 'Is there a member who wishes to speak on behalf of this Group to propose this?',\n",
       " 'Madam President, I can hear a ripple of laughter from the Socialists.',\n",
       " 'I was told that large sections of the Socialist Group were also keen to have this item taken off the agenda, because at the vote in the Conference of Presidents no vote was received from the working group of Members of the Socialist Group responsible for this matter.',\n",
       " 'I do not know whether this information is correct, but the PPE-DE Group would, in any case, be grateful if this item were removed because Parliament has addressed this issue several times already.',\n",
       " 'Decisions have also been adopted against a tax of this kind.',\n",
       " 'That is why my Group moves that this item be taken off the agenda.',\n",
       " 'Thank you, Mr Poettering.',\n",
       " 'We shall now hear Mr Wurtz speaking against this request.',\n",
       " \"Madam President, I would firstly like to point out Mr Poettering' s lack of logic. He has just been preaching to the Group of the Party of European Socialists because they went back on a decision taken in a perfectly clear manner at the Conference of Presidents, and now he is doing just the same.\",\n",
       " '',\n",
       " 'We discussed that matter and we were unanimous, with the exception of the PPE and ELDR Groups. As my fellow chairmen will recall, I even mentioned that it was not a matter of knowing whether one was for or against the Tobin tax, but of whether one dared to hear what the Commission and the Council thought of it.',\n",
       " 'It is not a lot to ask.',\n",
       " 'I therefore repeat the proposal that this oral question to the Commission and the Council should be retained so that we can find out, once and for all, the positions of these two bodies regarding the proposal which is relatively modest but which would give a clear message to public opinion, particularly after the tide of feeling generated by the failure of the Seattle Conference.',\n",
       " \"We shall proceed to vote on the PPE-DE Group' s request that the oral question regarding the capital tax be withdrawn from the agenda.\",\n",
       " '(Parliament rejected the request, with 164 votes for, 166 votes against and 7 abstentions)',\n",
       " 'Madam President, I would like to thank Mr Poettering for advertising this debate.',\n",
       " 'Thank you very much.',\n",
       " 'Madam President, has my vote been counted? I was unable to vote electronically, since I do not have a card.',\n",
       " 'My vote was \"in favour\" .$$$$$$',\n",
       " 'Madam President, the Presidency has already declared the result of the vote.',\n",
       " 'There is no room for amendments.',\n",
       " 'Madam President, in the earlier vote - and I will abide by your ruling on this matter - on the question of the strategic plan of the Commission I indicated that I would like to speak in advance of the vote on behalf of my Group.',\n",
       " 'That did not happen.',\n",
       " 'I would appreciate it if, on the close of this item of business, I might be allowed to give an explanation of vote on behalf of my Group.',\n",
       " 'This is an important matter.',\n",
       " 'It would be useful for the record of the House to state how people perceive what we have just done in the light of their own political analysis.',\n",
       " \"Madam President, I do not wish to reopen the debate, but I had also asked for the floor, to comment on Mr Barón Crespo's motion.\",\n",
       " 'You did not call me either.',\n",
       " 'I regret this, but the vote has already been taken and the decision is made so let us leave the matter there.',\n",
       " 'I am terribly sorry, Mr Hänsch and Mr Cox. I did not see you asking to speak.',\n",
       " 'Even so, I think the positions are quite clear and they shall be entered in the Minutes.',\n",
       " \"When we adopt the Minutes for today' s sitting tomorrow, then any Members who think the positions have not been explained clearly enough may ask for amendments.\",\n",
       " 'This seems to me to be a workable solution.',\n",
       " \"Of course, the Minutes for tomorrow' s sitting will take into account any additional explanations.\",\n",
       " 'I think this is a better solution than proceeding now to extremely time-consuming explanations of votes.',\n",
       " 'Mr Cox, Mr Hänsch, would this be acceptable to you?',\n",
       " 'Madam President, if the vote records correctly how my Group voted I shall not, and cannot, object to that.',\n",
       " 'If your ruling is that I cannot give an explanation of vote, I accept that but with reservations.',\n",
       " 'Safety advisers for the transport of dangerous goods',\n",
       " 'The next item is the report (A5-0105/1999) by Mr Koch, on behalf of the Committee on Regional Policy, Transport and Tourism, on the common position adopted by the Council with a view to adopting a European Parliament and Council directive on the harmonisation of examination requirements for safety advisers for the transport of dangerous goods by road, rail or inland waterways (C5-0208/1999 - 1998/0106(COD)).',\n",
       " \"Commissioner, Madam President, ladies and gentlemen, I can be quite frank in saying that I welcome the Council's common position on harmonising the training of safety advisers for the transport of dangerous goods by road, rail or inland waterway.\",\n",
       " 'Firstly, we needed to take action on a formal level in order to meet the requirements of Directive 96/35/EC, which obliges the Member States to appoint safety advisers and to organise the training, instruction and examination of these people but does not explain this explicitly.',\n",
       " 'Secondly, by adopting this directive we achieve a) an increase in safety when dangerous goods are both transported and transhipped; b) a reduction in distortions of competition resulting from wide variations in national training structures and training costs and c) equal opportunities for safety advisers on the European labour market.',\n",
       " \"Thirdly, this directive, as it currently stands in the common position, guarantees - in particular because it confines itself exclusively to minimum standards - a high degree of flexibility and modest regulation by the European Union; by adopting it we contribute to the Member States' bearing a high level of individual responsibility.\",\n",
       " 'All of this is in accordance with the principle of subsidiarity and is therefore to be greatly welcomed.',\n",
       " 'Our amendments from the first reading have, I believe, been taken into account very satisfactorily.',\n",
       " 'They have either been accepted or transposed with no change in the substance, or they have been rejected because the corresponding European arrangements have not been included, for example a system of penalties for violations of the rules or a complex classification structure for related groups of questions.',\n",
       " 'The one unanimously adopted amendment of the Committee on Regional Policy and Transport, which concerns the timetable for implementing the directive, is something which I would urge you to support.',\n",
       " 'By not setting a specific date for the Member States to implement the directive and instead giving them a period of three months after its entry into force, we are introducing a flexibility clause which ensures that the directive will be implemented without delay.',\n",
       " 'I would urge you to endorse this.',\n",
       " 'Madam President, we cannot and must not accept the fact that we hear ever more frequently of accidents causing major damage on our roads, but also on our railways and waterways, not solely but at least partly because those involved do not take the transport of dangerous goods seriously enough or because - as a result of ignorance or a lack of training on the part of the drivers or others responsible for the various vehicles - a minor accident has all too often become a major disaster.',\n",
       " 'As an Austrian, I still have a vivid memory, as, I believe, we all do, of the catastrophe which cost so many human lives last year in the Tauern Tunnel, where subsequent work to rebuild the parts of the tunnel which had been destroyed in this fire continued for many months at huge expense.',\n",
       " 'The renovation project, which lasted for months, cut off this important route between the north and south of Europe.',\n",
       " 'The traffic which had to be diverted because of this stretched the patience of many thousands of people in the EU to the limit.',\n",
       " 'In fact, all hell broke loose in some municipalities in my province.',\n",
       " 'Prevention has to be our answer to disasters of this kind and this draft directive is an important step towards well-trained safety advisers being available, so that the right action is taken in good time.',\n",
       " 'All the same, we must not content ourselves with enacting European law to ensure greater safety.',\n",
       " 'We also need to follow this up and make sure that our rules are transposed by the Member States in good time and - even more importantly - we need to ensure that they are also applied afterwards.',\n",
       " 'Please let this not be yet another sector where we subsequently have to lament the lack of enforcement.',\n",
       " 'I should like to address one final point. We must not content ourselves with sealing another hole in the safety net and shutting our eyes to the fact that, where transport safety in Europe is concerned, there is still much more to be done.',\n",
       " 'In this context, I should like to make a request and ask the Commissioner responsible, who is with us here today, to table an appropriate text as soon as possible with a view to continuing to make it safer for traffic to transit tunnels in the future, so that we in Europe do not have to experience any more such disasters on this scale.',\n",
       " 'Madam President, first of all I should like to thank Mr Koch for his report which has, at its heart, the issue of transport safety.',\n",
       " 'The report looks at the issue of harmonising the examination requirements for safety advisors working in the areas of transportation of dangerous goods by road, rail and inland waterway.',\n",
       " 'I congratulate him on his excellent report.',\n",
       " 'Transport safety has sadly been in the news recently: the Paddington rail crash in London, the terrible rail crash in Norway, the two aviation crashes involving EU citizens and the natural disaster involving the Erika off Brittany - all within the last four months - remind us that transport safety can never be taken for granted and that those charged with protecting the public must be highly motivated and highly qualified.',\n",
       " \"The rapporteur has pointed out to the House that in its common position the Council has accepted six of Parliament's ten amendments put forward at first reading and that the substance of Parliament's other amendments has been retained.\",\n",
       " 'My Group will therefore support the common position and looks forward to the enactment of the legislation which will provide us with yet another tool in our fight to make transport in the European Union as safe as possible.',\n",
       " 'When it comes to safety my Group will always support any initiatives to improve transport safety.',\n",
       " 'We still have a lot of work to do in this area as recent events have proved.',\n",
       " 'Madam President, I would like to make a few comments.',\n",
       " 'I would like, first of all, to thank the rapporteur for his exceptionally accurate and technical work on the report and, secondly, the Commission for the proposal it has submitted.',\n",
       " 'We are concerned here with the harmonisation of examination requirements but also, in fact, with minimum requirements.',\n",
       " 'This is a pity, in a sense.',\n",
       " 'Needless to say, safety on roads, railways and inland waterways is of key importance and, given the international nature of these types of transport, training for safety advisors should also be harmonised, therefore, as well as the requirements of the new ADR, for example, which is under way.',\n",
       " 'This is important, but so is enforcement and there are, of course, a number of reasons why we need to pay particular attention to this.',\n",
       " 'Just think of the road accidents which have occurred over recent years, for example in Belgium, the Netherlands and a number of other countries where lorries carrying dangerous goods continued to drive in foggy conditions when really they should have pulled off the road instead.',\n",
       " 'Or ships from Eastern Europe which moor adjacent to ships over here, with all the obvious risks that this entails.',\n",
       " 'Furthermore, it has transpired that research in the ports in Belgium, Finland, but also in Japan has shown that 50% of containers with partially dangerous cargo are not delivered correctly for shipment.',\n",
       " 'In short, the issue is an important one.',\n",
       " 'If we look at the situation where safety advisers are concerned, in a number of countries it is compulsory to employ such safety advisers in companies as from 1 January of this year.',\n",
       " 'There will be major problems with enforcing this rule at present, especially with smaller companies, as these cannot afford safety advisors.',\n",
       " 'These smaller companies either dispose of their cargo or mix it with other cargo, which causes problems.',\n",
       " 'It is therefore also being requested that ISO 9002 certificates possibly include the finer details of these activities in the form of annual reports and company analyses.',\n",
       " 'The work is done. All that remains is the business of enforcement.',\n",
       " 'I would like to mention one final point.',\n",
       " \"With regard to enforcement, proper agreements must also be concluded with the Eastern European countries because they will not enter into treaties which deal with this matter until 1 July 2001, that is to say in eighteen months' time.\",\n",
       " 'This gives them a competitive edge for the interim period.',\n",
       " 'This is not in itself anything dreadful, but we should prioritise particularly the safety aspects for goods transported by road, rail and inland waterways and incorporate these, as part of the acquis communautaire, as soon as possible and present them to the acceding states.',\n",
       " 'Madam President, the importance of transport safety is highlighted on a regular basis in this Parliament and rightly so.',\n",
       " 'The ever increasing volume of goods passing through Europe entails all kinds of risks, known and unknown, for employees and the social environment.',\n",
       " 'Those having to deal with these risks should therefore meet stringent requirements.',\n",
       " 'The relevant standards which have been laid down in another Directive, 95/35/EC, seem sufficiently adequate to advise people in a responsible manner on the organisation of the transport of dangerous goods.',\n",
       " 'I am very pleased that agreement has also been reached with the Council on minimum standards regarding examinations, although I would have preferred it if uniform, set standards and modules had been established, so that certificates would be of equal value internationally.',\n",
       " 'This, however, does not seem feasible.',\n",
       " 'Finally, the amendment tabled by the rapporteur is perfectly logical and I can, therefore, give it my wholehearted support.',\n",
       " 'Mr President, Commissioner, I should first like to congratulate Mr Koch on his reports which, though technical, are nonetheless of very great significance for safety.',\n",
       " 'I should like to make just a few comments.',\n",
       " 'Firstly, I should like to ask the Commissioner - and I am convinced that my request will fall on fertile ground - to ensure that more attention is paid to the issue of safety, be it on the roads, on the waterways or at sea.',\n",
       " 'Considering that it is only today that we are dealing with a Commission proposal first made on 19 March 1998, even though Parliament responded relatively quickly, this time lag is a little too long.',\n",
       " 'This is not just the fault of the Commission, but I believe that we need to take action more quickly so as to achieve harmonisation in this area as well.',\n",
       " 'My second point has already been mentioned: it concerns the minimum standards.',\n",
       " 'In principle, I believe that in many cases where transport is concerned we should be working towards increased flexibility and country-specific rules.',\n",
       " 'However, when it comes to safety, I am rather sceptical because safety in Sweden, for example, is in principle no different from safety in Germany, Italy or Austria.',\n",
       " 'I can live with these minimum standards, but I would ask the Commission to monitor the situation very carefully.',\n",
       " 'Should flexibility of this kind result in there being inadequate rules in some countries then we should work towards greater harmonisation.']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_en[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YUnKdtAGhgIr"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_en,target_vocab_size=2**14)\n",
    "tokenizer_es = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_es,target_vocab_size=2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69dnoHtoirZh"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\n",
    "VOCAB_SIZE_ES = tokenizer_es.vocab_size + 2\n",
    "# 2 is added as we add 2 new token i.e. for beggining and ending of sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_AMItRDk_S0"
   },
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_ES-2] + tokenizer_es.encode(sentence) + [VOCAB_SIZE_ES-1]\n",
    "           for sentence in corpus_es]\n",
    "#vocab_size_en-2 make it as orginal vocab size so we dont use that in encoding phase\n",
    "#ends at vocab_size - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbMl9ZFZp9sp"
   },
   "outputs": [],
   "source": [
    " #now we remove long sentences cause it requires huge computation power and time\n",
    "MAX_LEN = 20 \n",
    "idx_to_remove = [count for count,sen in enumerate(inputs) if len(sen)>MAX_LEN]\n",
    "\n",
    "for idx in reversed(idx_to_remove):\n",
    "    \n",
    "  #reverse order ma garnu parxa natra suru ko remove garda mathi ko sabai ko index ghatxa\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n",
    "idx_to_remove1 = [count for count,sen in enumerate(outputs) if len(sen)>MAX_LEN]\n",
    "\n",
    "for idx in reversed(idx_to_remove1):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GpWQ1Zlp-vu"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen = MAX_LEN)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OzwSVJ6nmFsY"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE =20000\n",
    "#create datasets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
    "dataset = dataset.cache() \n",
    "#does nothing to improve performance but increase speed of training by making data available easily\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#also just improves speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nyQuFpaGtDO"
   },
   "source": [
    "Positional encoding formulae:\n",
    "\n",
    "PE(pos,2i)=sin(pos/10000**(2i/dmodel)) \n",
    "\n",
    "PE(pos,2i+1)=cos(pos/10000**(2i/dmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G_3k9oOHDS0E"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "    #yesma just computation matra hunxa kei update garna pardaina so just super matra hunxa.\n",
    "    def get_angles(self,pos,i,d_model): #pos = [seq_model,1] and i=[1,d_model]\n",
    "    #pos and i are array\n",
    "        angles = 1/np.power(10000.,(2*(i//2))/np.float32(d_model))\n",
    "    #here in above formula we can see sine is for even and cos is for odd\n",
    "    #and i doesnt represent i but 2i in sin so we // by 2\n",
    "        return pos * angles #matrix of size (seq_length,d_model)\n",
    "  \n",
    "    def call(self,inputs):\n",
    "        seq_len = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_len)[:,np.newaxis],\n",
    "                             np.arange(d_model)[np.newaxis,:],\n",
    "                             d_model) #np.newaxis le dimension badhauxa say if (28,) xa vane (28,1)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis,...] # ... means takes all\n",
    "        return inputs + tf.cast(pos_encoding,tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-as1zi5ri8GK"
   },
   "outputs": [],
   "source": [
    "def scale_dot_product_attention(queries,keys,values,mask):\n",
    "    product = tf.matmul(queries,keys,transpose_b=True)\n",
    "\n",
    "    key_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "  # -1 gives last term i.e. embedding\n",
    "    scaled_product = product/ tf.math.sqrt(key_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "    scaled_product += (mask* -1e9)\n",
    "  #it was optional but still we put mask\n",
    "  # it is placed negative infinity such that when placed on softmax gives 0 i.e. no effect.\n",
    "\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product,axis=-1),values) #softmax is applied to last axis\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgqroqWXrNTV"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self,nb_proj):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "  #we require to define all _lin function and we know they all have same no. of units i.e. dmodel\n",
    "  #but we dont have acces to d_model\n",
    "  #so we can get that acess by defining build method\n",
    "  # which is similar to __init__ but instead of being called when object is initialized it is called when first time object is used.\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "    # assert demands to have that condition\n",
    "        self.d_proj = self.d_model// self.nb_proj\n",
    "        self.queries_lin = layers.Dense(units=self.d_model)\n",
    "        self.keys_lin = layers.Dense(units=self.d_model)\n",
    "        self.values_lin = layers.Dense(units=self.d_model)\n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "\n",
    "    def split_proj(self,inputs,batch_size): #inputs = [batch_size,seq_length,d_model]size\n",
    "        shaped = [batch_size,-1,self.nb_proj,self.d_proj] # -1 gives seq-len\n",
    "        splited_inputs = tf.reshape(inputs,shape=shaped) # [bat_size,seq_len,nb_proj,d_proj]\n",
    "        return tf.transpose(splited_inputs,perm = [0, 2, 1, 3]) #[bat_size,nb_proj,seq_len,d_proj]\n",
    " #d_proj = d_model//nb_proj\n",
    "    def call(self,queries,keys,values,mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        queries = self.queries_lin(queries)\n",
    "        keys = self.keys_lin(keys)\n",
    "        values = self.values_lin(values)\n",
    "\n",
    "        queries = self.split_proj(queries,batch_size)\n",
    "        keys = self.split_proj(keys,batch_size)\n",
    "        values = self.split_proj(values,batch_size)\n",
    "\n",
    "        attention = scale_dot_product_attention(queries, keys, values, mask)\n",
    "    # concat is exactly opposite to split proj   \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9r34naps8mL7"
   },
   "outputs": [],
   "source": [
    "class EncodingLayer(layers.Layer):\n",
    "    def __init__(self,FFN_units,nb_proj,dropout_rate):\n",
    "        super(EncodingLayer,self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate= self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon =1e-6)\n",
    "        self.dense_1 = layers.Dense(units= self.FFN_units,activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units = self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate= self.dropout_rate)\n",
    "\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "\n",
    "    def call(self,inputs,mask,training): #training is boolean here it is used for dropout as it is applied only while training\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                          inputs,\n",
    "                                          inputs,\n",
    "                                          mask)\n",
    "        attention = self.dropout_1(attention, training= training)\n",
    "        attention = self.norm_1(attention+inputs)\n",
    "\n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training= training)\n",
    "        outputs = self.norm_2(outputs+attention)\n",
    "\n",
    "         return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23JGIVl_T6Ps"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,nb_layers,FFN_units,nb_proj,dropout_rate,vocab_size,d_model,name=\"encoder\"):\n",
    "        super(Encoder,self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size,d_model)\n",
    "        self.positional_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.encoding_layers = [EncodingLayer(FFN_units,nb_proj,dropout_rate) for _ in range(nb_layers)]\n",
    "\n",
    "    def call(self,inputs,mask,training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    #just normalization ko lagi sqrt(d_model) le multiply garne vanethyo paper ma\n",
    "        outputs = self.positional_encoding(outputs)\n",
    "        outputs = self.dropout(outputs,training)\n",
    "\n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.encoding_layers[i](outputs,mask,training)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9q2whlObZsT4"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self,FFN_units,nb_proj,dropout_rate):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate = self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon =1e-6)\n",
    "\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate = self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon =1e-6)\n",
    "    \n",
    "        self.dense_1 = layers.Dense(units= self.FFN_units,activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units= self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate = self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon =1e-6)\n",
    "  \n",
    "    def call(self,inputs,enc_outputs,mask_1,mask_2,training):# 2 mask for 2 attention \n",
    "        attention1 = self.multi_head_attention_1(inputs,inputs,inputs,mask_1)\n",
    "        attention1 = self.dropout_1(attention1,training)\n",
    "        attention1 = self.norm_1(attention1+inputs)\n",
    "\n",
    "        attention2 = self.multi_head_attention_2(attention1,enc_outputs,enc_outputs,mask_2)\n",
    "        attention2 = self.dropout_2(attention2,training)\n",
    "        attention2 = self.norm_2(attention2+attention1)\n",
    "\n",
    "        outputs = self.dense_1(attention2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs,training)\n",
    "        outputs = self.norm_3(outputs + attention2)\n",
    "\n",
    "        return outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3C7r1YyWo8NW"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for i in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHvek6dir8Fb"
   },
   "outputs": [],
   "source": [
    "class Transformer(layers.Layer):\n",
    "    def __init__(self,vocab_size_enc,vocab_size_dec,d_model,nb_layers,FFN_units,nb_proj,\n",
    "               dropout_rate,name=\"transformer\"):\n",
    "        super(Transformer,self).__init__()  \n",
    "        self.encoder = Encoder(nb_layers,FFN_units,nb_proj,dropout_rate,vocab_size_enc,d_model)\n",
    "        self.decoder = Decoder(nb_layers,FFN_units,nb_proj,dropout_rate,vocab_size_dec,d_model)\n",
    "        self.last_linear = layers.Dense(units= vocab_size_dec,name=\"lin_output\")\n",
    "\n",
    "    def create_padding_mask(self,seq): #seq = [ batch_size,seq_len]\n",
    "        mask= tf.cast(tf.math.equal(seq,0),tf.float32)\n",
    "        return mask[:,tf.newaxis,tf.newaxis,:]\n",
    "    #here we add two dimension\n",
    "    #one for saying that mask is individually for sub-sequences\n",
    "    #another is just to make sure we can use row as many time as wanted\n",
    "\n",
    "    def create_look_ahead_mask(self,seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len,seq_len)),-1,0) #lower triangular matrix\n",
    "        return look_ahead_mask\n",
    "  # we use look a head matrix along with padding matrix with tf.maximum and it will automatically increase the dimension as required.\n",
    "\n",
    "    def call(self,enc_inputs,dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "\n",
    "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs),\n",
    "                            self.create_look_ahead_mask(dec_inputs))\n",
    "    \n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "\n",
    "        enc_outputs = self.encoder(enc_inputs,enc_mask, training)\n",
    "\n",
    "        dec_outputs = self.decoder(dec_inputs,enc_outputs,dec_mask_1,dec_mask_2,training)\n",
    "\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjMvRjVOwxMx"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.2# 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_ES,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          dropout_rate=DROPOUT_RATE,\n",
    "                          nb_proj=NB_PROJ\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p7AHMKBXw1SR"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "#reduction none means we dont want to take loss of all batch to represent by one numeric value\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    #first we want to get rid of losses from padding tokens\n",
    "    #above code we will get rid of all the loss =0 in targets i.e. padding tokens \n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "    #in this way we compute loss of only rreal number not the padding tokens\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IR2DeCN8xW_s"
   },
   "outputs": [],
   "source": [
    "# as mentioned in paper learning rate is self customized\n",
    "#so we customize as follow\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,d_model,warmup_steps=4000):\n",
    "        super(CustomSchedule,self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model,tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self,step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step* (self.warmup_steps**-1.5)\n",
    "    # since by default we put warmup_step = 4000 so for first 4000 arg2 is lower and follow them and then arg1  is lower\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1,arg2)\n",
    "\n",
    "    learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "# values are from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "upCY32bNxaoA"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./drive/My Drive/Transformer/ckpt/\"\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3Rgx-Gvx89hW",
    "outputId": "916dd4f5-c8f5-4a2a-cc03-7be376071b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Batch 0 Loss 1.6437 Accuracy 0.3692\n",
      "Epoch 1 Batch 50 Loss 1.5911 Accuracy 0.3914\n",
      "Epoch 1 Batch 100 Loss 1.5977 Accuracy 0.3944\n",
      "Epoch 1 Batch 150 Loss 1.5863 Accuracy 0.3942\n",
      "Epoch 1 Batch 200 Loss 1.5903 Accuracy 0.3955\n",
      "Epoch 1 Batch 250 Loss 1.5888 Accuracy 0.3960\n",
      "Epoch 1 Batch 300 Loss 1.5848 Accuracy 0.3957\n",
      "Epoch 1 Batch 350 Loss 1.5835 Accuracy 0.3964\n",
      "Epoch 1 Batch 400 Loss 1.5795 Accuracy 0.3957\n",
      "Epoch 1 Batch 450 Loss 1.5782 Accuracy 0.3958\n",
      "Epoch 1 Batch 500 Loss 1.5768 Accuracy 0.3957\n",
      "Epoch 1 Batch 550 Loss 1.5729 Accuracy 0.3962\n",
      "Epoch 1 Batch 600 Loss 1.5708 Accuracy 0.3963\n",
      "Epoch 1 Batch 650 Loss 1.5684 Accuracy 0.3964\n",
      "Epoch 1 Batch 700 Loss 1.5679 Accuracy 0.3960\n",
      "Epoch 1 Batch 750 Loss 1.5673 Accuracy 0.3962\n",
      "Epoch 1 Batch 800 Loss 1.5674 Accuracy 0.3963\n",
      "Epoch 1 Batch 850 Loss 1.5623 Accuracy 0.3967\n",
      "Epoch 1 Batch 900 Loss 1.5601 Accuracy 0.3972\n",
      "Epoch 1 Batch 950 Loss 1.5543 Accuracy 0.3978\n",
      "Epoch 1 Batch 1000 Loss 1.5500 Accuracy 0.3991\n",
      "Epoch 1 Batch 1050 Loss 1.5432 Accuracy 0.4003\n",
      "Epoch 1 Batch 1100 Loss 1.5377 Accuracy 0.4015\n",
      "Epoch 1 Batch 1150 Loss 1.5308 Accuracy 0.4028\n",
      "Epoch 1 Batch 1200 Loss 1.5237 Accuracy 0.4041\n",
      "Epoch 1 Batch 1250 Loss 1.5173 Accuracy 0.4049\n",
      "Epoch 1 Batch 1300 Loss 1.5117 Accuracy 0.4055\n",
      "Epoch 1 Batch 1350 Loss 1.5065 Accuracy 0.4061\n",
      "Epoch 1 Batch 1400 Loss 1.5014 Accuracy 0.4068\n",
      "Epoch 1 Batch 1450 Loss 1.4965 Accuracy 0.4075\n",
      "Epoch 1 Batch 1500 Loss 1.4914 Accuracy 0.4083\n",
      "Epoch 1 Batch 1550 Loss 1.4863 Accuracy 0.4092\n",
      "Epoch 1 Batch 1600 Loss 1.4806 Accuracy 0.4101\n",
      "Epoch 1 Batch 1650 Loss 1.4749 Accuracy 0.4110\n",
      "Epoch 1 Batch 1700 Loss 1.4698 Accuracy 0.4117\n",
      "Epoch 1 Batch 1750 Loss 1.4651 Accuracy 0.4122\n",
      "Epoch 1 Batch 1800 Loss 1.4611 Accuracy 0.4130\n",
      "Epoch 1 Batch 1850 Loss 1.4563 Accuracy 0.4137\n",
      "Epoch 1 Batch 1900 Loss 1.4521 Accuracy 0.4143\n",
      "Epoch 1 Batch 1950 Loss 1.4472 Accuracy 0.4151\n",
      "Epoch 1 Batch 2000 Loss 1.4427 Accuracy 0.4157\n",
      "Epoch 1 Batch 2050 Loss 1.4382 Accuracy 0.4163\n",
      "Epoch 1 Batch 2100 Loss 1.4337 Accuracy 0.4169\n",
      "Epoch 1 Batch 2150 Loss 1.4300 Accuracy 0.4173\n",
      "Epoch 1 Batch 2200 Loss 1.4266 Accuracy 0.4176\n",
      "Epoch 1 Batch 2250 Loss 1.4231 Accuracy 0.4182\n",
      "Epoch 1 Batch 2300 Loss 1.4198 Accuracy 0.4187\n",
      "Epoch 1 Batch 2350 Loss 1.4161 Accuracy 0.4190\n",
      "Epoch 1 Batch 2400 Loss 1.4124 Accuracy 0.4194\n",
      "Epoch 1 Batch 2450 Loss 1.4089 Accuracy 0.4198\n",
      "Epoch 1 Batch 2500 Loss 1.4057 Accuracy 0.4202\n",
      "Epoch 1 Batch 2550 Loss 1.4024 Accuracy 0.4208\n",
      "Epoch 1 Batch 2600 Loss 1.3995 Accuracy 0.4212\n",
      "Epoch 1 Batch 2650 Loss 1.3967 Accuracy 0.4215\n",
      "Epoch 1 Batch 2700 Loss 1.3938 Accuracy 0.4219\n",
      "Epoch 1 Batch 2750 Loss 1.3903 Accuracy 0.4223\n",
      "Epoch 1 Batch 2800 Loss 1.3871 Accuracy 0.4227\n",
      "Epoch 1 Batch 2850 Loss 1.3839 Accuracy 0.4230\n",
      "Epoch 1 Batch 2900 Loss 1.3808 Accuracy 0.4233\n",
      "Epoch 1 Batch 2950 Loss 1.3770 Accuracy 0.4235\n",
      "Epoch 1 Batch 3000 Loss 1.3743 Accuracy 0.4238\n",
      "Epoch 1 Batch 3050 Loss 1.3714 Accuracy 0.4241\n",
      "Epoch 1 Batch 3100 Loss 1.3689 Accuracy 0.4244\n",
      "Epoch 1 Batch 3150 Loss 1.3665 Accuracy 0.4246\n",
      "Epoch 1 Batch 3200 Loss 1.3639 Accuracy 0.4249\n",
      "Epoch 1 Batch 3250 Loss 1.3620 Accuracy 0.4252\n",
      "Epoch 1 Batch 3300 Loss 1.3595 Accuracy 0.4255\n",
      "Epoch 1 Batch 3350 Loss 1.3576 Accuracy 0.4258\n",
      "Epoch 1 Batch 3400 Loss 1.3556 Accuracy 0.4260\n",
      "Epoch 1 Batch 3450 Loss 1.3543 Accuracy 0.4263\n",
      "Epoch 1 Batch 3500 Loss 1.3524 Accuracy 0.4267\n",
      "Epoch 1 Batch 3550 Loss 1.3507 Accuracy 0.4270\n",
      "Epoch 1 Batch 3600 Loss 1.3494 Accuracy 0.4273\n",
      "Epoch 1 Batch 3650 Loss 1.3480 Accuracy 0.4276\n",
      "Epoch 1 Batch 3700 Loss 1.3465 Accuracy 0.4279\n",
      "Epoch 1 Batch 3750 Loss 1.3455 Accuracy 0.4281\n",
      "Epoch 1 Batch 3800 Loss 1.3437 Accuracy 0.4284\n",
      "Epoch 1 Batch 3850 Loss 1.3421 Accuracy 0.4288\n",
      "Epoch 1 Batch 3900 Loss 1.3407 Accuracy 0.4291\n",
      "Epoch 1 Batch 3950 Loss 1.3393 Accuracy 0.4294\n",
      "Epoch 1 Batch 4000 Loss 1.3382 Accuracy 0.4297\n",
      "Epoch 1 Batch 4050 Loss 1.3365 Accuracy 0.4299\n",
      "Epoch 1 Batch 4100 Loss 1.3354 Accuracy 0.4302\n",
      "Epoch 1 Batch 4150 Loss 1.3342 Accuracy 0.4305\n",
      "Epoch 1 Batch 4200 Loss 1.3327 Accuracy 0.4309\n",
      "Epoch 1 Batch 4250 Loss 1.3312 Accuracy 0.4312\n",
      "Epoch 1 Batch 4300 Loss 1.3298 Accuracy 0.4314\n",
      "Epoch 1 Batch 4350 Loss 1.3277 Accuracy 0.4317\n",
      "Epoch 1 Batch 4400 Loss 1.3259 Accuracy 0.4319\n",
      "Epoch 1 Batch 4450 Loss 1.3243 Accuracy 0.4322\n",
      "Epoch 1 Batch 4500 Loss 1.3229 Accuracy 0.4325\n",
      "Epoch 1 Batch 4550 Loss 1.3216 Accuracy 0.4327\n",
      "Epoch 1 Batch 4600 Loss 1.3198 Accuracy 0.4330\n",
      "Epoch 1 Batch 4650 Loss 1.3183 Accuracy 0.4333\n",
      "Epoch 1 Batch 4700 Loss 1.3171 Accuracy 0.4336\n",
      "Epoch 1 Batch 4750 Loss 1.3158 Accuracy 0.4340\n",
      "Epoch 1 Batch 4800 Loss 1.3143 Accuracy 0.4344\n",
      "Epoch 1 Batch 4850 Loss 1.3127 Accuracy 0.4348\n",
      "Epoch 1 Batch 4900 Loss 1.3110 Accuracy 0.4352\n",
      "Epoch 1 Batch 4950 Loss 1.3094 Accuracy 0.4356\n",
      "Epoch 1 Batch 5000 Loss 1.3081 Accuracy 0.4360\n",
      "Epoch 1 Batch 5050 Loss 1.3068 Accuracy 0.4364\n",
      "Epoch 1 Batch 5100 Loss 1.3051 Accuracy 0.4368\n",
      "Epoch 1 Batch 5150 Loss 1.3038 Accuracy 0.4371\n",
      "Epoch 1 Batch 5200 Loss 1.3025 Accuracy 0.4375\n",
      "Epoch 1 Batch 5250 Loss 1.3011 Accuracy 0.4378\n",
      "Epoch 1 Batch 5300 Loss 1.2995 Accuracy 0.4381\n",
      "Epoch 1 Batch 5350 Loss 1.2981 Accuracy 0.4384\n",
      "Epoch 1 Batch 5400 Loss 1.2972 Accuracy 0.4387\n",
      "Epoch 1 Batch 5450 Loss 1.2970 Accuracy 0.4388\n",
      "Epoch 1 Batch 5500 Loss 1.2973 Accuracy 0.4388\n",
      "Epoch 1 Batch 5550 Loss 1.2980 Accuracy 0.4388\n",
      "Epoch 1 Batch 5600 Loss 1.2991 Accuracy 0.4387\n",
      "Epoch 1 Batch 5650 Loss 1.3005 Accuracy 0.4386\n",
      "Epoch 1 Batch 5700 Loss 1.3019 Accuracy 0.4385\n",
      "Epoch 1 Batch 5750 Loss 1.3034 Accuracy 0.4383\n",
      "Epoch 1 Batch 5800 Loss 1.3052 Accuracy 0.4381\n",
      "Epoch 1 Batch 5850 Loss 1.3070 Accuracy 0.4379\n",
      "Epoch 1 Batch 5900 Loss 1.3090 Accuracy 0.4377\n",
      "Epoch 1 Batch 5950 Loss 1.3109 Accuracy 0.4374\n",
      "Epoch 1 Batch 6000 Loss 1.3129 Accuracy 0.4372\n",
      "Epoch 1 Batch 6050 Loss 1.3149 Accuracy 0.4369\n",
      "Epoch 1 Batch 6100 Loss 1.3169 Accuracy 0.4367\n",
      "Epoch 1 Batch 6150 Loss 1.3187 Accuracy 0.4364\n",
      "Epoch 1 Batch 6200 Loss 1.3206 Accuracy 0.4362\n",
      "Epoch 1 Batch 6250 Loss 1.3224 Accuracy 0.4360\n",
      "Epoch 1 Batch 6300 Loss 1.3242 Accuracy 0.4357\n",
      "Epoch 1 Batch 6350 Loss 1.3259 Accuracy 0.4355\n",
      "Epoch 1 Batch 6400 Loss 1.3275 Accuracy 0.4353\n",
      "Epoch 1 Batch 6450 Loss 1.3292 Accuracy 0.4350\n",
      "Epoch 1 Batch 6500 Loss 1.3310 Accuracy 0.4348\n",
      "Epoch 1 Batch 6550 Loss 1.3327 Accuracy 0.4346\n",
      "Epoch 1 Batch 6600 Loss 1.3343 Accuracy 0.4344\n",
      "Epoch 1 Batch 6650 Loss 1.3359 Accuracy 0.4341\n",
      "Epoch 1 Batch 6700 Loss 1.3377 Accuracy 0.4338\n",
      "Epoch 1 Batch 6750 Loss 1.3392 Accuracy 0.4335\n",
      "Epoch 1 Batch 6800 Loss 1.3409 Accuracy 0.4333\n",
      "Epoch 1 Batch 6850 Loss 1.3424 Accuracy 0.4331\n",
      "Epoch 1 Batch 6900 Loss 1.3436 Accuracy 0.4328\n",
      "Epoch 1 Batch 6950 Loss 1.3451 Accuracy 0.4325\n",
      "Epoch 1 Batch 7000 Loss 1.3465 Accuracy 0.4322\n",
      "Epoch 1 Batch 7050 Loss 1.3479 Accuracy 0.4320\n",
      "Epoch 1 Batch 7100 Loss 1.3491 Accuracy 0.4317\n",
      "Epoch 1 Batch 7150 Loss 1.3505 Accuracy 0.4315\n",
      "Epoch 1 Batch 7200 Loss 1.3516 Accuracy 0.4312\n",
      "Epoch 1 Batch 7250 Loss 1.3526 Accuracy 0.4310\n",
      "Epoch 1 Batch 7300 Loss 1.3536 Accuracy 0.4308\n",
      "Epoch 1 Batch 7350 Loss 1.3546 Accuracy 0.4306\n",
      "Saving checkpoint for epoch 1 at ./drive/My Drive/Transformer/ckpt/ckpt-4\n",
      "Time taken for 1 epoch: 2254.800568819046 secs\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Batch 0 Loss 1.4949 Accuracy 0.3725\n",
      "Epoch 2 Batch 50 Loss 1.5228 Accuracy 0.3950\n",
      "Epoch 2 Batch 100 Loss 1.5134 Accuracy 0.4006\n",
      "Epoch 2 Batch 150 Loss 1.5135 Accuracy 0.4002\n",
      "Epoch 2 Batch 200 Loss 1.5170 Accuracy 0.4022\n",
      "Epoch 2 Batch 250 Loss 1.5192 Accuracy 0.4038\n",
      "Epoch 2 Batch 300 Loss 1.5189 Accuracy 0.4035\n",
      "Epoch 2 Batch 350 Loss 1.5136 Accuracy 0.4037\n",
      "Epoch 2 Batch 400 Loss 1.5132 Accuracy 0.4034\n",
      "Epoch 2 Batch 450 Loss 1.5108 Accuracy 0.4036\n",
      "Epoch 2 Batch 500 Loss 1.5150 Accuracy 0.4041\n",
      "Epoch 2 Batch 550 Loss 1.5116 Accuracy 0.4041\n",
      "Epoch 2 Batch 600 Loss 1.5099 Accuracy 0.4042\n",
      "Epoch 2 Batch 650 Loss 1.5082 Accuracy 0.4042\n",
      "Epoch 2 Batch 700 Loss 1.5063 Accuracy 0.4041\n",
      "Epoch 2 Batch 750 Loss 1.5043 Accuracy 0.4039\n",
      "Epoch 2 Batch 800 Loss 1.5043 Accuracy 0.4035\n",
      "Epoch 2 Batch 850 Loss 1.5017 Accuracy 0.4038\n",
      "Epoch 2 Batch 900 Loss 1.5005 Accuracy 0.4045\n",
      "Epoch 2 Batch 950 Loss 1.4969 Accuracy 0.4056\n",
      "Epoch 2 Batch 1000 Loss 1.4925 Accuracy 0.4066\n",
      "Epoch 2 Batch 1050 Loss 1.4902 Accuracy 0.4077\n",
      "Epoch 2 Batch 1100 Loss 1.4852 Accuracy 0.4089\n",
      "Epoch 2 Batch 1150 Loss 1.4786 Accuracy 0.4100\n",
      "Epoch 2 Batch 1200 Loss 1.4719 Accuracy 0.4110\n",
      "Epoch 2 Batch 1250 Loss 1.4655 Accuracy 0.4119\n",
      "Epoch 2 Batch 1300 Loss 1.4593 Accuracy 0.4129\n",
      "Epoch 2 Batch 1350 Loss 1.4524 Accuracy 0.4138\n",
      "Epoch 2 Batch 1400 Loss 1.4460 Accuracy 0.4147\n",
      "Epoch 2 Batch 1450 Loss 1.4397 Accuracy 0.4154\n",
      "Epoch 2 Batch 1500 Loss 1.4340 Accuracy 0.4162\n",
      "Epoch 2 Batch 1550 Loss 1.4286 Accuracy 0.4169\n",
      "Epoch 2 Batch 1600 Loss 1.4218 Accuracy 0.4178\n",
      "Epoch 2 Batch 1650 Loss 1.4168 Accuracy 0.4187\n",
      "Epoch 2 Batch 1700 Loss 1.4122 Accuracy 0.4194\n",
      "Epoch 2 Batch 1750 Loss 1.4076 Accuracy 0.4202\n",
      "Epoch 2 Batch 1800 Loss 1.4027 Accuracy 0.4208\n",
      "Epoch 2 Batch 1850 Loss 1.3985 Accuracy 0.4217\n",
      "Epoch 2 Batch 1900 Loss 1.3934 Accuracy 0.4223\n",
      "Epoch 2 Batch 1950 Loss 1.3894 Accuracy 0.4230\n",
      "Epoch 2 Batch 2000 Loss 1.3851 Accuracy 0.4237\n",
      "Epoch 2 Batch 2050 Loss 1.3812 Accuracy 0.4242\n",
      "Epoch 2 Batch 2100 Loss 1.3769 Accuracy 0.4248\n",
      "Epoch 2 Batch 2150 Loss 1.3725 Accuracy 0.4253\n",
      "Epoch 2 Batch 2200 Loss 1.3684 Accuracy 0.4257\n",
      "Epoch 2 Batch 2250 Loss 1.3659 Accuracy 0.4262\n",
      "Epoch 2 Batch 2300 Loss 1.3620 Accuracy 0.4267\n",
      "Epoch 2 Batch 2350 Loss 1.3584 Accuracy 0.4270\n",
      "Epoch 2 Batch 2400 Loss 1.3550 Accuracy 0.4275\n",
      "Epoch 2 Batch 2450 Loss 1.3517 Accuracy 0.4278\n",
      "Epoch 2 Batch 2500 Loss 1.3478 Accuracy 0.4281\n",
      "Epoch 2 Batch 2550 Loss 1.3448 Accuracy 0.4285\n",
      "Epoch 2 Batch 2600 Loss 1.3414 Accuracy 0.4289\n",
      "Epoch 2 Batch 2650 Loss 1.3382 Accuracy 0.4293\n",
      "Epoch 2 Batch 2700 Loss 1.3358 Accuracy 0.4296\n",
      "Epoch 2 Batch 2750 Loss 1.3327 Accuracy 0.4300\n",
      "Epoch 2 Batch 2800 Loss 1.3300 Accuracy 0.4304\n",
      "Epoch 2 Batch 2850 Loss 1.3266 Accuracy 0.4307\n",
      "Epoch 2 Batch 2900 Loss 1.3234 Accuracy 0.4309\n",
      "Epoch 2 Batch 2950 Loss 1.3199 Accuracy 0.4312\n",
      "Epoch 2 Batch 3000 Loss 1.3167 Accuracy 0.4315\n",
      "Epoch 2 Batch 3050 Loss 1.3136 Accuracy 0.4318\n",
      "Epoch 2 Batch 3100 Loss 1.3114 Accuracy 0.4321\n",
      "Epoch 2 Batch 3150 Loss 1.3088 Accuracy 0.4325\n",
      "Epoch 2 Batch 3200 Loss 1.3065 Accuracy 0.4327\n",
      "Epoch 2 Batch 3250 Loss 1.3047 Accuracy 0.4330\n",
      "Epoch 2 Batch 3300 Loss 1.3026 Accuracy 0.4333\n",
      "Epoch 2 Batch 3350 Loss 1.3009 Accuracy 0.4336\n",
      "Epoch 2 Batch 3400 Loss 1.2991 Accuracy 0.4339\n",
      "Epoch 2 Batch 3450 Loss 1.2977 Accuracy 0.4342\n",
      "Epoch 2 Batch 3500 Loss 1.2961 Accuracy 0.4344\n",
      "Epoch 2 Batch 3550 Loss 1.2943 Accuracy 0.4346\n",
      "Epoch 2 Batch 3600 Loss 1.2925 Accuracy 0.4350\n",
      "Epoch 2 Batch 3650 Loss 1.2908 Accuracy 0.4353\n",
      "Epoch 2 Batch 3700 Loss 1.2895 Accuracy 0.4355\n",
      "Epoch 2 Batch 3750 Loss 1.2880 Accuracy 0.4359\n",
      "Epoch 2 Batch 3800 Loss 1.2865 Accuracy 0.4362\n",
      "Epoch 2 Batch 3850 Loss 1.2847 Accuracy 0.4365\n",
      "Epoch 2 Batch 3900 Loss 1.2835 Accuracy 0.4367\n",
      "Epoch 2 Batch 3950 Loss 1.2823 Accuracy 0.4370\n",
      "Epoch 2 Batch 4000 Loss 1.2812 Accuracy 0.4373\n",
      "Epoch 2 Batch 4050 Loss 1.2803 Accuracy 0.4375\n",
      "Epoch 2 Batch 4100 Loss 1.2792 Accuracy 0.4378\n",
      "Epoch 2 Batch 4150 Loss 1.2781 Accuracy 0.4381\n",
      "Epoch 2 Batch 4200 Loss 1.2770 Accuracy 0.4384\n",
      "Epoch 2 Batch 4250 Loss 1.2753 Accuracy 0.4387\n",
      "Epoch 2 Batch 4300 Loss 1.2740 Accuracy 0.4389\n",
      "Epoch 2 Batch 4350 Loss 1.2724 Accuracy 0.4392\n",
      "Epoch 2 Batch 4400 Loss 1.2710 Accuracy 0.4395\n",
      "Epoch 2 Batch 4450 Loss 1.2696 Accuracy 0.4397\n",
      "Epoch 2 Batch 4500 Loss 1.2684 Accuracy 0.4400\n",
      "Epoch 2 Batch 4550 Loss 1.2672 Accuracy 0.4403\n",
      "Epoch 2 Batch 4600 Loss 1.2657 Accuracy 0.4406\n",
      "Epoch 2 Batch 4650 Loss 1.2644 Accuracy 0.4409\n",
      "Epoch 2 Batch 4700 Loss 1.2628 Accuracy 0.4413\n",
      "Epoch 2 Batch 4750 Loss 1.2613 Accuracy 0.4416\n",
      "Epoch 2 Batch 4800 Loss 1.2596 Accuracy 0.4420\n",
      "Epoch 2 Batch 4850 Loss 1.2580 Accuracy 0.4423\n",
      "Epoch 2 Batch 4900 Loss 1.2564 Accuracy 0.4427\n",
      "Epoch 2 Batch 4950 Loss 1.2547 Accuracy 0.4431\n",
      "Epoch 2 Batch 5000 Loss 1.2533 Accuracy 0.4434\n",
      "Epoch 2 Batch 5050 Loss 1.2519 Accuracy 0.4438\n",
      "Epoch 2 Batch 5100 Loss 1.2505 Accuracy 0.4442\n",
      "Epoch 2 Batch 5150 Loss 1.2491 Accuracy 0.4445\n",
      "Epoch 2 Batch 5200 Loss 1.2478 Accuracy 0.4448\n",
      "Epoch 2 Batch 5250 Loss 1.2462 Accuracy 0.4452\n",
      "Epoch 2 Batch 5300 Loss 1.2448 Accuracy 0.4455\n",
      "Epoch 2 Batch 5350 Loss 1.2435 Accuracy 0.4459\n",
      "Epoch 2 Batch 5400 Loss 1.2424 Accuracy 0.4462\n",
      "Epoch 2 Batch 5450 Loss 1.2422 Accuracy 0.4463\n",
      "Epoch 2 Batch 5500 Loss 1.2425 Accuracy 0.4463\n",
      "Epoch 2 Batch 5550 Loss 1.2434 Accuracy 0.4463\n",
      "Epoch 2 Batch 5600 Loss 1.2448 Accuracy 0.4462\n",
      "Epoch 2 Batch 5650 Loss 1.2462 Accuracy 0.4461\n",
      "Epoch 2 Batch 5700 Loss 1.2475 Accuracy 0.4459\n",
      "Epoch 2 Batch 5750 Loss 1.2494 Accuracy 0.4458\n",
      "Epoch 2 Batch 5800 Loss 1.2512 Accuracy 0.4456\n",
      "Epoch 2 Batch 5850 Loss 1.2532 Accuracy 0.4454\n",
      "Epoch 2 Batch 5900 Loss 1.2551 Accuracy 0.4451\n",
      "Epoch 2 Batch 5950 Loss 1.2574 Accuracy 0.4449\n",
      "Epoch 2 Batch 6000 Loss 1.2595 Accuracy 0.4447\n",
      "Epoch 2 Batch 6050 Loss 1.2616 Accuracy 0.4444\n",
      "Epoch 2 Batch 6100 Loss 1.2635 Accuracy 0.4441\n",
      "Epoch 2 Batch 6150 Loss 1.2654 Accuracy 0.4439\n",
      "Epoch 2 Batch 6200 Loss 1.2673 Accuracy 0.4436\n",
      "Epoch 2 Batch 6250 Loss 1.2692 Accuracy 0.4434\n",
      "Epoch 2 Batch 6300 Loss 1.2708 Accuracy 0.4431\n",
      "Epoch 2 Batch 6350 Loss 1.2725 Accuracy 0.4429\n",
      "Epoch 2 Batch 6400 Loss 1.2741 Accuracy 0.4426\n",
      "Epoch 2 Batch 6450 Loss 1.2757 Accuracy 0.4424\n",
      "Epoch 2 Batch 6500 Loss 1.2774 Accuracy 0.4422\n",
      "Epoch 2 Batch 6550 Loss 1.2793 Accuracy 0.4419\n",
      "Epoch 2 Batch 6600 Loss 1.2810 Accuracy 0.4417\n",
      "Epoch 2 Batch 6650 Loss 1.2828 Accuracy 0.4414\n",
      "Epoch 2 Batch 6700 Loss 1.2848 Accuracy 0.4412\n",
      "Epoch 2 Batch 6750 Loss 1.2865 Accuracy 0.4409\n",
      "Epoch 2 Batch 6800 Loss 1.2882 Accuracy 0.4406\n",
      "Epoch 2 Batch 6850 Loss 1.2897 Accuracy 0.4403\n",
      "Epoch 2 Batch 6900 Loss 1.2912 Accuracy 0.4400\n",
      "Epoch 2 Batch 6950 Loss 1.2927 Accuracy 0.4397\n",
      "Epoch 2 Batch 7000 Loss 1.2940 Accuracy 0.4394\n",
      "Epoch 2 Batch 7050 Loss 1.2952 Accuracy 0.4391\n",
      "Epoch 2 Batch 7100 Loss 1.2966 Accuracy 0.4389\n",
      "Epoch 2 Batch 7150 Loss 1.2979 Accuracy 0.4386\n",
      "Epoch 2 Batch 7200 Loss 1.2991 Accuracy 0.4384\n",
      "Epoch 2 Batch 7250 Loss 1.3006 Accuracy 0.4381\n",
      "Epoch 2 Batch 7300 Loss 1.3020 Accuracy 0.4379\n",
      "Epoch 2 Batch 7350 Loss 1.3031 Accuracy 0.4377\n",
      "Saving checkpoint for epoch 2 at ./drive/My Drive/Transformer/ckpt/ckpt-5\n",
      "Time taken for 1 epoch: 2213.5600345134735 secs\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.4456 Accuracy 0.3939\n",
      "Epoch 3 Batch 50 Loss 1.5046 Accuracy 0.4038\n",
      "Epoch 3 Batch 100 Loss 1.5050 Accuracy 0.4056\n",
      "Epoch 3 Batch 150 Loss 1.4866 Accuracy 0.4065\n",
      "Epoch 3 Batch 200 Loss 1.4789 Accuracy 0.4075\n",
      "Epoch 3 Batch 250 Loss 1.4843 Accuracy 0.4081\n",
      "Epoch 3 Batch 300 Loss 1.4824 Accuracy 0.4080\n",
      "Epoch 3 Batch 350 Loss 1.4808 Accuracy 0.4080\n",
      "Epoch 3 Batch 400 Loss 1.4794 Accuracy 0.4083\n",
      "Epoch 3 Batch 450 Loss 1.4780 Accuracy 0.4085\n",
      "Epoch 3 Batch 500 Loss 1.4755 Accuracy 0.4088\n",
      "Epoch 3 Batch 550 Loss 1.4726 Accuracy 0.4090\n",
      "Epoch 3 Batch 600 Loss 1.4697 Accuracy 0.4088\n",
      "Epoch 3 Batch 650 Loss 1.4681 Accuracy 0.4091\n",
      "Epoch 3 Batch 700 Loss 1.4677 Accuracy 0.4094\n",
      "Epoch 3 Batch 750 Loss 1.4657 Accuracy 0.4095\n",
      "Epoch 3 Batch 800 Loss 1.4671 Accuracy 0.4095\n",
      "Epoch 3 Batch 850 Loss 1.4662 Accuracy 0.4097\n",
      "Epoch 3 Batch 900 Loss 1.4634 Accuracy 0.4103\n",
      "Epoch 3 Batch 950 Loss 1.4600 Accuracy 0.4113\n",
      "Epoch 3 Batch 1000 Loss 1.4552 Accuracy 0.4122\n",
      "Epoch 3 Batch 1050 Loss 1.4503 Accuracy 0.4132\n",
      "Epoch 3 Batch 1100 Loss 1.4438 Accuracy 0.4144\n",
      "Epoch 3 Batch 1150 Loss 1.4371 Accuracy 0.4156\n",
      "Epoch 3 Batch 1200 Loss 1.4305 Accuracy 0.4168\n",
      "Epoch 3 Batch 1250 Loss 1.4250 Accuracy 0.4176\n",
      "Epoch 3 Batch 1300 Loss 1.4185 Accuracy 0.4183\n",
      "Epoch 3 Batch 1350 Loss 1.4120 Accuracy 0.4192\n",
      "Epoch 3 Batch 1400 Loss 1.4058 Accuracy 0.4200\n",
      "Epoch 3 Batch 1450 Loss 1.4000 Accuracy 0.4208\n",
      "Epoch 3 Batch 1500 Loss 1.3941 Accuracy 0.4217\n",
      "Epoch 3 Batch 1550 Loss 1.3877 Accuracy 0.4226\n",
      "Epoch 3 Batch 1600 Loss 1.3817 Accuracy 0.4234\n",
      "Epoch 3 Batch 1650 Loss 1.3767 Accuracy 0.4244\n",
      "Epoch 3 Batch 1700 Loss 1.3723 Accuracy 0.4253\n",
      "Epoch 3 Batch 1750 Loss 1.3676 Accuracy 0.4260\n",
      "Epoch 3 Batch 1800 Loss 1.3621 Accuracy 0.4266\n",
      "Epoch 3 Batch 1850 Loss 1.3576 Accuracy 0.4273\n",
      "Epoch 3 Batch 1900 Loss 1.3529 Accuracy 0.4280\n",
      "Epoch 3 Batch 1950 Loss 1.3486 Accuracy 0.4286\n",
      "Epoch 3 Batch 2000 Loss 1.3444 Accuracy 0.4292\n",
      "Epoch 3 Batch 2050 Loss 1.3396 Accuracy 0.4297\n",
      "Epoch 3 Batch 2100 Loss 1.3362 Accuracy 0.4303\n",
      "Epoch 3 Batch 2150 Loss 1.3320 Accuracy 0.4307\n",
      "Epoch 3 Batch 2200 Loss 1.3283 Accuracy 0.4312\n",
      "Epoch 3 Batch 2250 Loss 1.3248 Accuracy 0.4318\n",
      "Epoch 3 Batch 2300 Loss 1.3210 Accuracy 0.4322\n",
      "Epoch 3 Batch 2350 Loss 1.3173 Accuracy 0.4326\n",
      "Epoch 3 Batch 2400 Loss 1.3133 Accuracy 0.4331\n",
      "Epoch 3 Batch 2450 Loss 1.3103 Accuracy 0.4335\n",
      "Epoch 3 Batch 2500 Loss 1.3065 Accuracy 0.4340\n",
      "Epoch 3 Batch 2550 Loss 1.3032 Accuracy 0.4344\n",
      "Epoch 3 Batch 2600 Loss 1.3000 Accuracy 0.4348\n",
      "Epoch 3 Batch 2650 Loss 1.2968 Accuracy 0.4352\n",
      "Epoch 3 Batch 2700 Loss 1.2947 Accuracy 0.4357\n",
      "Epoch 3 Batch 2750 Loss 1.2917 Accuracy 0.4360\n",
      "Epoch 3 Batch 2800 Loss 1.2887 Accuracy 0.4363\n",
      "Epoch 3 Batch 2850 Loss 1.2850 Accuracy 0.4367\n",
      "Epoch 3 Batch 2900 Loss 1.2824 Accuracy 0.4370\n",
      "Epoch 3 Batch 2950 Loss 1.2794 Accuracy 0.4372\n",
      "Epoch 3 Batch 3000 Loss 1.2763 Accuracy 0.4376\n",
      "Epoch 3 Batch 3050 Loss 1.2736 Accuracy 0.4378\n",
      "Epoch 3 Batch 3100 Loss 1.2710 Accuracy 0.4380\n",
      "Epoch 3 Batch 3150 Loss 1.2686 Accuracy 0.4383\n",
      "Epoch 3 Batch 3200 Loss 1.2666 Accuracy 0.4385\n",
      "Epoch 3 Batch 3250 Loss 1.2649 Accuracy 0.4388\n",
      "Epoch 3 Batch 3300 Loss 1.2630 Accuracy 0.4390\n",
      "Epoch 3 Batch 3350 Loss 1.2610 Accuracy 0.4392\n",
      "Epoch 3 Batch 3400 Loss 1.2589 Accuracy 0.4395\n",
      "Epoch 3 Batch 3450 Loss 1.2569 Accuracy 0.4397\n",
      "Epoch 3 Batch 3500 Loss 1.2552 Accuracy 0.4401\n",
      "Epoch 3 Batch 3550 Loss 1.2539 Accuracy 0.4403\n",
      "Epoch 3 Batch 3600 Loss 1.2525 Accuracy 0.4406\n",
      "Epoch 3 Batch 3650 Loss 1.2512 Accuracy 0.4409\n",
      "Epoch 3 Batch 3700 Loss 1.2498 Accuracy 0.4412\n",
      "Epoch 3 Batch 3750 Loss 1.2483 Accuracy 0.4415\n",
      "Epoch 3 Batch 3800 Loss 1.2470 Accuracy 0.4417\n",
      "Epoch 3 Batch 3850 Loss 1.2457 Accuracy 0.4421\n",
      "Epoch 3 Batch 3900 Loss 1.2442 Accuracy 0.4423\n",
      "Epoch 3 Batch 3950 Loss 1.2430 Accuracy 0.4426\n",
      "Epoch 3 Batch 4000 Loss 1.2419 Accuracy 0.4429\n",
      "Epoch 3 Batch 4050 Loss 1.2405 Accuracy 0.4432\n",
      "Epoch 3 Batch 4100 Loss 1.2392 Accuracy 0.4435\n",
      "Epoch 3 Batch 4150 Loss 1.2381 Accuracy 0.4437\n",
      "Epoch 3 Batch 4200 Loss 1.2366 Accuracy 0.4440\n",
      "Epoch 3 Batch 4250 Loss 1.2354 Accuracy 0.4443\n",
      "Epoch 3 Batch 4300 Loss 1.2339 Accuracy 0.4445\n",
      "Epoch 3 Batch 4350 Loss 1.2325 Accuracy 0.4448\n",
      "Epoch 3 Batch 4400 Loss 1.2314 Accuracy 0.4451\n",
      "Epoch 3 Batch 4450 Loss 1.2301 Accuracy 0.4454\n",
      "Epoch 3 Batch 4500 Loss 1.2287 Accuracy 0.4456\n",
      "Epoch 3 Batch 4550 Loss 1.2274 Accuracy 0.4458\n",
      "Epoch 3 Batch 4600 Loss 1.2259 Accuracy 0.4462\n",
      "Epoch 3 Batch 4650 Loss 1.2246 Accuracy 0.4465\n",
      "Epoch 3 Batch 4700 Loss 1.2232 Accuracy 0.4468\n",
      "Epoch 3 Batch 4750 Loss 1.2218 Accuracy 0.4471\n",
      "Epoch 3 Batch 4800 Loss 1.2203 Accuracy 0.4474\n",
      "Epoch 3 Batch 4850 Loss 1.2188 Accuracy 0.4478\n",
      "Epoch 3 Batch 4900 Loss 1.2174 Accuracy 0.4481\n",
      "Epoch 3 Batch 4950 Loss 1.2159 Accuracy 0.4485\n",
      "Epoch 3 Batch 5000 Loss 1.2147 Accuracy 0.4489\n",
      "Epoch 3 Batch 5050 Loss 1.2132 Accuracy 0.4492\n",
      "Epoch 3 Batch 5100 Loss 1.2117 Accuracy 0.4495\n",
      "Epoch 3 Batch 5150 Loss 1.2107 Accuracy 0.4499\n",
      "Epoch 3 Batch 5200 Loss 1.2094 Accuracy 0.4502\n",
      "Epoch 3 Batch 5250 Loss 1.2081 Accuracy 0.4505\n",
      "Epoch 3 Batch 5300 Loss 1.2069 Accuracy 0.4509\n",
      "Epoch 3 Batch 5350 Loss 1.2056 Accuracy 0.4513\n",
      "Epoch 3 Batch 5400 Loss 1.2048 Accuracy 0.4516\n",
      "Epoch 3 Batch 5450 Loss 1.2048 Accuracy 0.4517\n",
      "Epoch 3 Batch 5500 Loss 1.2052 Accuracy 0.4518\n",
      "Epoch 3 Batch 5550 Loss 1.2059 Accuracy 0.4517\n",
      "Epoch 3 Batch 5600 Loss 1.2072 Accuracy 0.4516\n",
      "Epoch 3 Batch 5650 Loss 1.2086 Accuracy 0.4515\n",
      "Epoch 3 Batch 5700 Loss 1.2102 Accuracy 0.4513\n",
      "Epoch 3 Batch 5750 Loss 1.2119 Accuracy 0.4511\n",
      "Epoch 3 Batch 5800 Loss 1.2137 Accuracy 0.4508\n",
      "Epoch 3 Batch 5850 Loss 1.2157 Accuracy 0.4506\n",
      "Epoch 3 Batch 5900 Loss 1.2180 Accuracy 0.4504\n",
      "Epoch 3 Batch 5950 Loss 1.2201 Accuracy 0.4501\n",
      "Epoch 3 Batch 6000 Loss 1.2220 Accuracy 0.4499\n",
      "Epoch 3 Batch 6050 Loss 1.2240 Accuracy 0.4496\n",
      "Epoch 3 Batch 6100 Loss 1.2261 Accuracy 0.4493\n",
      "Epoch 3 Batch 6150 Loss 1.2280 Accuracy 0.4490\n",
      "Epoch 3 Batch 6200 Loss 1.2300 Accuracy 0.4488\n",
      "Epoch 3 Batch 6250 Loss 1.2318 Accuracy 0.4485\n",
      "Epoch 3 Batch 6300 Loss 1.2337 Accuracy 0.4483\n",
      "Epoch 3 Batch 6350 Loss 1.2355 Accuracy 0.4480\n",
      "Epoch 3 Batch 6400 Loss 1.2373 Accuracy 0.4478\n",
      "Epoch 3 Batch 6450 Loss 1.2390 Accuracy 0.4476\n",
      "Epoch 3 Batch 6500 Loss 1.2409 Accuracy 0.4473\n",
      "Epoch 3 Batch 6550 Loss 1.2428 Accuracy 0.4471\n",
      "Epoch 3 Batch 6600 Loss 1.2448 Accuracy 0.4469\n",
      "Epoch 3 Batch 6650 Loss 1.2464 Accuracy 0.4466\n",
      "Epoch 3 Batch 6700 Loss 1.2480 Accuracy 0.4463\n",
      "Epoch 3 Batch 6750 Loss 1.2497 Accuracy 0.4460\n",
      "Epoch 3 Batch 6800 Loss 1.2516 Accuracy 0.4458\n",
      "Epoch 3 Batch 6850 Loss 1.2531 Accuracy 0.4455\n",
      "Epoch 3 Batch 6900 Loss 1.2546 Accuracy 0.4452\n",
      "Epoch 3 Batch 6950 Loss 1.2561 Accuracy 0.4448\n",
      "Epoch 3 Batch 7000 Loss 1.2575 Accuracy 0.4446\n",
      "Epoch 3 Batch 7050 Loss 1.2587 Accuracy 0.4443\n",
      "Epoch 3 Batch 7100 Loss 1.2600 Accuracy 0.4440\n",
      "Epoch 3 Batch 7150 Loss 1.2612 Accuracy 0.4438\n",
      "Epoch 3 Batch 7200 Loss 1.2625 Accuracy 0.4435\n",
      "Epoch 3 Batch 7250 Loss 1.2640 Accuracy 0.4432\n",
      "Epoch 3 Batch 7300 Loss 1.2653 Accuracy 0.4430\n",
      "Epoch 3 Batch 7350 Loss 1.2668 Accuracy 0.4427\n",
      "Saving checkpoint for epoch 3 at ./drive/My Drive/Transformer/ckpt/ckpt-6\n",
      "Time taken for 1 epoch: 2184.3567984104156 secs\n",
      "\n",
      "Start of epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.4855 Accuracy 0.4104\n",
      "Epoch 4 Batch 50 Loss 1.4577 Accuracy 0.4129\n",
      "Epoch 4 Batch 100 Loss 1.4540 Accuracy 0.4141\n",
      "Epoch 4 Batch 150 Loss 1.4516 Accuracy 0.4132\n",
      "Epoch 4 Batch 200 Loss 1.4626 Accuracy 0.4120\n",
      "Epoch 4 Batch 250 Loss 1.4574 Accuracy 0.4125\n",
      "Epoch 4 Batch 300 Loss 1.4524 Accuracy 0.4133\n",
      "Epoch 4 Batch 350 Loss 1.4467 Accuracy 0.4134\n",
      "Epoch 4 Batch 400 Loss 1.4458 Accuracy 0.4135\n",
      "Epoch 4 Batch 450 Loss 1.4472 Accuracy 0.4141\n",
      "Epoch 4 Batch 500 Loss 1.4431 Accuracy 0.4138\n",
      "Epoch 4 Batch 550 Loss 1.4415 Accuracy 0.4137\n",
      "Epoch 4 Batch 600 Loss 1.4393 Accuracy 0.4141\n",
      "Epoch 4 Batch 650 Loss 1.4348 Accuracy 0.4137\n",
      "Epoch 4 Batch 700 Loss 1.4350 Accuracy 0.4137\n",
      "Epoch 4 Batch 750 Loss 1.4329 Accuracy 0.4141\n",
      "Epoch 4 Batch 800 Loss 1.4335 Accuracy 0.4142\n",
      "Epoch 4 Batch 850 Loss 1.4320 Accuracy 0.4145\n",
      "Epoch 4 Batch 900 Loss 1.4296 Accuracy 0.4150\n",
      "Epoch 4 Batch 950 Loss 1.4266 Accuracy 0.4161\n",
      "Epoch 4 Batch 1000 Loss 1.4223 Accuracy 0.4171\n",
      "Epoch 4 Batch 1050 Loss 1.4168 Accuracy 0.4181\n",
      "Epoch 4 Batch 1100 Loss 1.4116 Accuracy 0.4194\n",
      "Epoch 4 Batch 1150 Loss 1.4055 Accuracy 0.4205\n",
      "Epoch 4 Batch 1200 Loss 1.3987 Accuracy 0.4217\n",
      "Epoch 4 Batch 1250 Loss 1.3917 Accuracy 0.4223\n",
      "Epoch 4 Batch 1300 Loss 1.3860 Accuracy 0.4233\n",
      "Epoch 4 Batch 1350 Loss 1.3804 Accuracy 0.4240\n",
      "Epoch 4 Batch 1400 Loss 1.3748 Accuracy 0.4247\n",
      "Epoch 4 Batch 1450 Loss 1.3693 Accuracy 0.4254\n",
      "Epoch 4 Batch 1500 Loss 1.3644 Accuracy 0.4263\n",
      "Epoch 4 Batch 1550 Loss 1.3589 Accuracy 0.4271\n",
      "Epoch 4 Batch 1600 Loss 1.3521 Accuracy 0.4279\n",
      "Epoch 4 Batch 1650 Loss 1.3463 Accuracy 0.4285\n",
      "Epoch 4 Batch 1700 Loss 1.3409 Accuracy 0.4293\n",
      "Epoch 4 Batch 1750 Loss 1.3361 Accuracy 0.4300\n",
      "Epoch 4 Batch 1800 Loss 1.3321 Accuracy 0.4305\n",
      "Epoch 4 Batch 1850 Loss 1.3273 Accuracy 0.4312\n",
      "Epoch 4 Batch 1900 Loss 1.3227 Accuracy 0.4321\n",
      "Epoch 4 Batch 1950 Loss 1.3185 Accuracy 0.4328\n",
      "Epoch 4 Batch 2000 Loss 1.3137 Accuracy 0.4333\n",
      "Epoch 4 Batch 2050 Loss 1.3102 Accuracy 0.4339\n",
      "Epoch 4 Batch 2100 Loss 1.3059 Accuracy 0.4344\n",
      "Epoch 4 Batch 2150 Loss 1.3015 Accuracy 0.4350\n",
      "Epoch 4 Batch 2200 Loss 1.2979 Accuracy 0.4356\n",
      "Epoch 4 Batch 2250 Loss 1.2942 Accuracy 0.4359\n",
      "Epoch 4 Batch 2300 Loss 1.2912 Accuracy 0.4362\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    #reset loss and accuracy before each epochs\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "      # send in tuples as dataset have inputs and targets\n",
    "        dec_inputs = targets[:, :-1] \n",
    "        # all targets except last\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        # excluding 1st one as it is starting tokens\n",
    "        with tf.GradientTape() as tape:\n",
    "          #GradientTape is used to track all the info while training\n",
    "            predictions = transformer(enc_inputs, dec_inputs, training=True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E0Ji_Je1M9_p"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence = \\\n",
    "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
    "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "    # to simulate batch size we add dim at axis =0\n",
    "    output = tf.expand_dims([VOCAB_SIZE_ES-2], axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions = transformer(enc_input, output, False)\n",
    "        #no dropout apply to testing so False\n",
    "        prediction = predictions[:, -1:, :]\n",
    "        #we take from last term so -1:\n",
    "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "        \n",
    "        if predicted_id == VOCAB_SIZE_ES-1:\n",
    "          #if predicted id is end token of spanish word\n",
    "            return tf.squeeze(output, axis=0)\n",
    "        # we cant directly return output as we have added dimension for batch size\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        \n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBSCuk-kNu6K"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output = evaluate(sentence).numpy()\n",
    "    # sentence is encoded so  we directly evaluate\n",
    "    # it will be in tensor so for our convinent we convert it to numpy\n",
    "    predicted_sentence = tokenizer_fr.decode(\n",
    "        [i for i in output if i < VOCAB_SIZE_ES-2]\n",
    "    )\n",
    "    # output is in encoded term so we decode it but not all , only real sentences token\n",
    "    \n",
    "    print(\"Input: {}\".format(sentence))\n",
    "    print(\"Predicted translation: {}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fWFaKAIvPfrs"
   },
   "outputs": [],
   "source": [
    "translate(\"This is a really powerful tool!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AImmCHLJPyWt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
